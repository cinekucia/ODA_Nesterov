{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class DualGradientMethod:\n",
    "    def __init__(self, A, b, penalty, gamma_u, gamma_d, L_0, v_0, max_iter=10000, tol=1e-6) -> None:\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.penalty = penalty\n",
    "        self.gamma_u = gamma_u\n",
    "        self.gamma_d = gamma_d\n",
    "        self.L = L_0\n",
    "        self.v = v_0\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.gap_history = []\n",
    "        self.loss_history = []\n",
    "        self.cpu_time_history = []\n",
    "\n",
    "    def gradient_f(self, x):\n",
    "        return 2 * np.dot(self.A.T, (np.dot(self.A, x) - self.b))\n",
    "\n",
    "    def f(self, x):\n",
    "        return np.linalg.norm(np.dot(self.A, x) - self.b)**2 / 2\n",
    "\n",
    "    def psi(self, x):\n",
    "        # Placeholder for potential regularizer function\n",
    "        return 0\n",
    "\n",
    "    def compute_steps(self):\n",
    "        v_prev = self.v\n",
    "        initial_residual = np.dot(self.A, self.v) - self.b\n",
    "        initial_loss = np.linalg.norm(initial_residual)**2 / 2\n",
    "        \n",
    "        for k in range(self.max_iter):\n",
    "            start_time = time.process_time()\n",
    "\n",
    "            # Nesterov acceleration\n",
    "            if k > 0:\n",
    "                y = self.v + (k - 1) / (k + 2) * (self.v - v_prev)\n",
    "            else:\n",
    "                y = self.v\n",
    "\n",
    "            grad_f = self.gradient_f(y)\n",
    "            Mk = np.linalg.norm(grad_f, 2)\n",
    "            if Mk == 0:\n",
    "                Mk = 1e-16  # Prevent division by zero\n",
    "\n",
    "            L_k = max(self.L, Mk / self.gamma_d)\n",
    "            T, L_new = self.gradient_iteration(self.penalty, self.gamma_u, y, L_k)\n",
    "            self.L = L_new\n",
    "            \n",
    "            v_prev = self.v\n",
    "            self.v = T\n",
    "            \n",
    "            current_residual = np.dot(self.A, self.v) - self.b\n",
    "            current_loss = np.linalg.norm(current_residual)**2 / 2\n",
    "            current_gap = current_loss / initial_loss\n",
    "\n",
    "            end_time = time.process_time()\n",
    "\n",
    "            self.gap_history.append(current_gap)\n",
    "            self.loss_history.append(current_loss)\n",
    "            self.cpu_time_history.append(end_time - start_time)\n",
    "\n",
    "            if current_gap < self.tol:\n",
    "                break\n",
    "\n",
    "        return self.v, k, current_gap, self.gap_history, self.loss_history, self.cpu_time_history\n",
    "\n",
    "    def gradient_iteration(self, penalty, gamma_u, x, M):\n",
    "        L = M\n",
    "        while True:\n",
    "            changes = self.gradient_f(x) - L * x\n",
    "            T = three_cases(changes, penalty, L)\n",
    "            if not self.psi(T) > np.dot(self.gradient_f(T), (x - T)) + np.linalg.norm(x - T)**2 * L / 2 + self.psi(x):\n",
    "                break\n",
    "            L *= gamma_u\n",
    "        return T, L\n",
    "\n",
    "def three_cases(changes, penalty, denominator):\n",
    "    # Vectorized handling of three cases for the proximal gradient update\n",
    "    return (np.less_equal(changes, -penalty) * (-changes - penalty) + np.greater_equal(changes, penalty) * (-changes + penalty)) / denominator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [-0.4449078  -0.12895728  0.          0.          0.        ]...\n",
      "Iterations: 9999\n",
      "Final Gap: 9.121210451221099e-07\n",
      "Indices of significant gap changes: [   1    2    3    5    6    7    8    9   11   12   13   16   20   24\n",
      "   26   29   37   40   42   65 1339]\n",
      "CPU time at significant changes: [ 0.03125   0.09375   0.09375   0.15625   0.171875  0.234375  0.234375\n",
      "  0.25      0.34375   0.375     0.40625   0.46875   0.609375  0.78125\n",
      "  0.84375   0.921875  1.234375  1.296875  1.34375   2.125    25.625   ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def generate_sparse_least_squares(m, n, rho):\n",
    "    \"\"\"\n",
    "    Generates a Sparse Least Squares problem according to the specifications in Nesterov's paper.\n",
    "\n",
    "    Parameters:\n",
    "    m (int): Number of rows in matrix A.\n",
    "    n (int): Number of columns in matrix A, n > m.\n",
    "    rho (float): Sparsity and magnitude control parameter.\n",
    "\n",
    "    Returns:\n",
    "    A (numpy.ndarray): Generated dense matrix A of shape (m, n).\n",
    "    b (numpy.ndarray): Generated vector b of shape (m,).\n",
    "    x_star (numpy.ndarray): Sparse solution x* of shape (n,).\n",
    "\n",
    "    Example:\n",
    "    m, n, rho = 10, 20, 0.1  # dimensions and sparsity level\n",
    "    A, b, x_star = generate_sparse_least_squares(m, n, rho)\n",
    "\n",
    "    # print(\"Matrix A:\\n\", A)\n",
    "    # print(\"Vector b:\\n\", b)\n",
    "    print(\"Sparse solution x*:\\n\", x_star)\n",
    "    \"\"\"\n",
    "    # Generate a dense matrix A with elements uniformly distributed in [-1, 1]\n",
    "    A = np.random.uniform(-1, 1, (m, n))\n",
    "\n",
    "    # Generate a sparse solution x_star\n",
    "    x_star = np.zeros(n)\n",
    "    # Ensure sparsity in the solution\n",
    "    non_zero_indices = np.random.choice(n, int(n * rho), replace=False)\n",
    "    x_star[non_zero_indices] = np.random.normal(0, 1, int(n * rho))\n",
    "\n",
    "    # Calculate the vector b = A*x_star + noise\n",
    "    # noise = np.random.normal(0, 0.1, m)  # adding small Gaussian noise\n",
    "    b = np.dot(A, x_star) # + noise\n",
    "\n",
    "    return A, b, x_star\n",
    "\n",
    "# Parameters\n",
    "m, n = 1000, 4000  # dimensions\n",
    "rho = 0.1          # sparsity factor\n",
    "tol = 1e-9         # tolerance for relative gap reduction\n",
    "\n",
    "# Generate the problem\n",
    "A, b, x_star = generate_sparse_least_squares(m, n, rho)\n",
    "\n",
    "# Instantiate and solve using DualGradientMethod\n",
    "v0 = np.zeros(n)\n",
    "L0 = 2\n",
    "gamma_d = 2\n",
    "penalty = 0.5  # This replaces lambda_ which was likely a regularization parameter\n",
    "\n",
    "solver = DualGradientMethod(A, b, penalty, gamma_d=2, gamma_u=1.1, L_0=L0, v_0=v0, max_iter=10000, tol=tol)\n",
    "solution_dual, iterations_dual, final_gap_dual, gap_history_dual, loss_history_dual, cpu_history_dual = solver.compute_steps()\n",
    "\n",
    "# Analyzing gap magnitudes and CPU time\n",
    "def find_magnitude_indices(gap_history, func=np.log10):\n",
    "    magnitudes = np.floor(func(gap_history))\n",
    "    unique_magnitudes = []\n",
    "    indices = []\n",
    "    for i, magnitude in enumerate(magnitudes):\n",
    "        if magnitude not in unique_magnitudes:\n",
    "            unique_magnitudes.append(magnitude)\n",
    "            indices.append(i + 1)\n",
    "    indices = np.array(indices)\n",
    "    return indices, unique_magnitudes\n",
    "\n",
    "indices_dual, magnitudes_dual = find_magnitude_indices(gap_history_dual, func=np.log2)\n",
    "cpu_history_dual = np.array(cpu_history_dual).cumsum()[indices_dual - 1]  # Adjusting indices for Python's 0-based index\n",
    "\n",
    "# Output results for analysis\n",
    "print(f\"Solution: {solution_dual[:5]}...\")  # Showing first few elements of the solution\n",
    "print(f\"Iterations: {iterations_dual}\")\n",
    "print(f\"Final Gap: {final_gap_dual}\")\n",
    "print(f\"Indices of significant gap changes: {indices_dual}\")\n",
    "print(f\"CPU time at significant changes: {cpu_history_dual}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [-0.05443786 -0.18406338 -0.0289667   0.27226145  0.05750593]...\n",
      "Iterations: 9999\n",
      "Final Gap: 4.741904856153861e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class PrimalGradientMethod:\n",
    "    def __init__(self, A, b, x0, penalty, gamma_u, L_0, tol, max_iter=10000) -> None:\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.x = x0\n",
    "        self.penalty = penalty\n",
    "        self.gamma_u = gamma_u\n",
    "        self.gamma_d = 1 / gamma_u  # As gamma_d seems to be the inverse process in gradient_iteration\n",
    "        self.L = L_0\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.gap_history = []\n",
    "        self.loss_history = []\n",
    "        self.cpu_time_history = []\n",
    "\n",
    "    def gradient_f(self, x):\n",
    "        \"\"\"Computes the gradient of the least squares function.\"\"\"\n",
    "        return 2 * np.dot(self.A.T, (np.dot(self.A, x) - self.b))\n",
    "\n",
    "    def f(self, x):\n",
    "        \"\"\"Computes the loss (norm of residual squared).\"\"\"\n",
    "        return np.linalg.norm(np.dot(self.A, x) - self.b)**2\n",
    "\n",
    "    def compute_steps(self):\n",
    "        initial_loss = self.f(self.x)\n",
    "        self.loss_history.append(initial_loss)\n",
    "        \n",
    "        for k in range(self.max_iter):\n",
    "            start_time = time.process_time()\n",
    "\n",
    "            gradient = self.gradient_f(self.x)\n",
    "            step_size = 0.1 / (np.linalg.norm(gradient) if np.linalg.norm(gradient) != 0 else 1e-16)\n",
    "            self.x -= step_size * gradient\n",
    "\n",
    "            current_loss = self.f(self.x)\n",
    "            relative_gap = current_loss / initial_loss\n",
    "\n",
    "            self.loss_history.append(current_loss)\n",
    "            self.gap_history.append(relative_gap)\n",
    "            end_time = time.process_time()\n",
    "            self.cpu_time_history.append(end_time - start_time)\n",
    "\n",
    "            if relative_gap < self.tol:\n",
    "                break\n",
    "\n",
    "        return self.x, k, relative_gap, self.gap_history, self.loss_history, self.cpu_time_history\n",
    "\n",
    "# Example Usage\n",
    "# Parameters\n",
    "m, n = 1000, 4000  # dimensions\n",
    "rho = 0.1          # sparsity factor\n",
    "tol = 1e-9         # tolerance for relative gap reduction\n",
    "\n",
    "# Generate the problem\n",
    "A, b, x_star = generate_sparse_least_squares(m, n, rho)\n",
    "x0 = np.zeros(n)\n",
    "penalty = 0.5\n",
    "gamma_u = 1.1\n",
    "L_0 = 2\n",
    "tol = 1e-6\n",
    "max_iter = 10000\n",
    "\n",
    "solver = PrimalGradientMethod(A, b, x0, penalty, gamma_u, L_0, tol, max_iter)\n",
    "solution, iterations, final_gap, gap_history, loss_history, cpu_time = solver.compute_steps()\n",
    "\n",
    "print(f\"Solution: {solution[:5]}...\")  # Showing first few elements of the solution\n",
    "print(f\"Iterations: {iterations}\")\n",
    "print(f\"Final Gap: {final_gap}\")\n",
    "\n",
    "print(f\"Solution: {solution_dual[:5]}...\")  # Showing first few elements of the solution\n",
    "print(f\"Iterations: {iterations_dual}\")\n",
    "print(f\"Final Gap: {final_gap_dual}\")\n",
    "print(f\"Indices of significant gap changes: {indices_dual}\")\n",
    "print(f\"CPU time at significant changes: {cpu_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [ 0.17145659 -0.00623629 -0.04250588  0.16693029 -0.1020973 ]...\n",
      "Iterations: 9999\n",
      "Final Gap: 0.00027067967329087316\n",
      "Indices of significant gap changes: [ 1 15 27 35 42 47 50 53 56 57 58 59 60 61 63 64]\n",
      "CPU time at significant changes: [0.       0.109375 0.21875  0.3125   0.375    0.421875 0.421875 0.4375\n",
      " 0.46875  0.46875  0.46875  0.484375 0.484375 0.484375 0.484375 0.484375]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "\n",
    "def generate_sparse_least_squares(m, n, rho):\n",
    "    \"\"\"\n",
    "    Generate a sparse least squares problem where `A` is an m x n matrix,\n",
    "    `b` is an m-dimensional vector, and `x_star` is a sparse n-dimensional vector.\n",
    "    \"\"\"\n",
    "    A = np.random.randn(m, n)\n",
    "    x_star = sp.rand(n, 1, density=rho).toarray().ravel()  # n-dimensional, sparsity defined by rho\n",
    "    b = np.dot(A, x_star) + np.random.randn(m) * 0.1  # Adding some noise\n",
    "    return A, b, x_star\n",
    "\n",
    "class PrimalGradientMethod:\n",
    "    def __init__(self, A, b, x0, tol, max_iter=10000):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.x = x0\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.gap_history = []\n",
    "        self.loss_history = []\n",
    "        self.cpu_time_history = []\n",
    "\n",
    "    def gradient_f(self, x):\n",
    "        return 2 * np.dot(self.A.T, (np.dot(self.A, x) - self.b))\n",
    "\n",
    "    def f(self, x):\n",
    "        return np.linalg.norm(np.dot(self.A, x) - self.b)**2\n",
    "\n",
    "    def compute_steps(self):\n",
    "        initial_loss = self.f(self.x)\n",
    "        self.loss_history.append(initial_loss)\n",
    "        \n",
    "        for k in range(self.max_iter):\n",
    "            start_time = time.process_time()\n",
    "            gradient = self.gradient_f(self.x)\n",
    "            step_size = 0.1 / (np.linalg.norm(gradient) if np.linalg.norm(gradient) != 0 else 1e-16)\n",
    "            self.x -= step_size * gradient\n",
    "            current_loss = self.f(self.x)\n",
    "            relative_gap = current_loss / initial_loss\n",
    "\n",
    "            self.loss_history.append(current_loss)\n",
    "            self.gap_history.append(relative_gap)\n",
    "            end_time = time.process_time()\n",
    "            self.cpu_time_history.append(end_time - start_time)\n",
    "\n",
    "            if relative_gap < self.tol:\n",
    "                break\n",
    "\n",
    "        return self.x, k, relative_gap, self.gap_history, self.loss_history, self.cpu_time_history\n",
    "\n",
    "# Parameters\n",
    "m, n = 1000, 4000  # dimensions\n",
    "rho = 0.1          # sparsity factor\n",
    "tol = 1e-9         # tolerance for relative gap reduction\n",
    "\n",
    "# Generate the problem\n",
    "A, b, x_star = generate_sparse_least_squares(m, n, rho)\n",
    "x0 = np.zeros(n)\n",
    "\n",
    "# Instantiate and solve using PrimalGradientMethod\n",
    "solver = PrimalGradientMethod(A, b, x0, tol, max_iter=10000)\n",
    "solution, iterations, final_gap, gap_history, loss_history, cpu_time = solver.compute_steps()\n",
    "\n",
    "# Analyzing gap magnitudes and CPU time\n",
    "def find_magnitude_indices(gap_history, func=np.log2):\n",
    "    magnitudes = np.floor(func(gap_history))\n",
    "    unique_magnitudes = []\n",
    "    indices = []\n",
    "    for i, magnitude in enumerate(magnitudes):\n",
    "        if magnitude not in unique_magnitudes:\n",
    "            unique_magnitudes.append(magnitude)\n",
    "            indices.append(i + 1)\n",
    "    indices = np.array(indices)\n",
    "    return indices, unique_magnitudes\n",
    "\n",
    "indices, magnitudes = find_magnitude_indices(gap_history)\n",
    "cpu_time = np.array(cpu_time).cumsum()[indices - 1]  # Adjusting indices for Python's 0-based index\n",
    "\n",
    "# Output results for analysis\n",
    "print(f\"Solution: {solution[:5]}...\")  # Showing first few elements of the solution\n",
    "print(f\"Iterations: {iterations}\")\n",
    "print(f\"Final Gap: {final_gap}\")\n",
    "print(f\"Indices of significant gap changes: {indices}\")\n",
    "print(f\"CPU time at significant changes: {cpu_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [ 0.03043061  0.         -0.00111708  0.00376453 -0.0003996 ]...\n",
      "Loss history: [60.64288288181856, 59.74762287190232, 58.70750855252287, 57.651181306609, 56.65804949290718]...\n",
      "Gap history: [0.13086156823504916, 0.7044536615267238, 0.6306883361071357, 0.38739959263401974, 0.3220556553039628]...\n",
      "CPU time history: [0.015625, 0.015625, 0.015625, 0.0, 0.015625]...\n",
      "Solution: [ 0.03043061  0.         -0.00111708  0.00376453 -0.0003996 ]...\n",
      "Iterations: 9999\n",
      "Final Gap: 0.00027067967329087316\n",
      "Indices of significant gap changes: [ 1 15 27 35 42 47 50 53 56 57 58 59 60 61 63 64]\n",
      "CPU time at significant changes: [0.       0.109375 0.21875  0.3125   0.375    0.421875 0.421875 0.4375\n",
      " 0.46875  0.46875  0.46875  0.484375 0.484375 0.484375 0.484375 0.484375]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from math import sqrt\n",
    "\n",
    "def soft_thresholding(x, lambda_):\n",
    "    \"\"\" Apply soft thresholding for LASSO. \"\"\"\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - lambda_, 0.0)\n",
    "\n",
    "def lasso_objective(X, y, beta, lambda_):\n",
    "    \"\"\" Compute LASSO objective function. \"\"\"\n",
    "    n = len(y)\n",
    "    residual = y - X.dot(beta)\n",
    "    loss = 0.5 * np.dot(residual, residual) / n\n",
    "    penalty = lambda_ * np.linalg.norm(beta, ord=1)\n",
    "    return loss + penalty\n",
    "\n",
    "def lasso_gradient(X, y, beta, lambda_):\n",
    "    \"\"\" Compute gradient for LASSO. \"\"\"\n",
    "    n = len(y)\n",
    "    residual = y - X.dot(beta)\n",
    "    gradient_loss = -X.T.dot(residual) / n\n",
    "    gradient_penalty = lambda_ * np.sign(beta)\n",
    "    return gradient_loss + gradient_penalty\n",
    "\n",
    "class AcceleratedLassoMethod:\n",
    "    def __init__(self, X, y, lambda_, lr=0.01, max_iter=1000, tol=1e-6):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.lambda_ = lambda_\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.beta = np.zeros(X.shape[1])\n",
    "        self.t = 1\n",
    "        self.loss_history = []\n",
    "        self.gap_history = []\n",
    "        self.cpu_time_history = []\n",
    "\n",
    "    def compute_steps(self):\n",
    "        beta_prev = np.zeros_like(self.beta)\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            start_time = time.process_time()\n",
    "            t_prev = self.t\n",
    "            self.t = 0.5 * (1 + sqrt(1 + 4 * t_prev**2))\n",
    "            y_tilde = self.beta + ((t_prev - 1) / self.t) * (self.beta - beta_prev)\n",
    "            gradient = lasso_gradient(self.X, self.y, y_tilde, self.lambda_)\n",
    "            beta_prev = self.beta.copy()\n",
    "            self.beta = soft_thresholding(y_tilde - self.lr * gradient, self.lr * self.lambda_)\n",
    "            loss = lasso_objective(self.X, self.y, self.beta, self.lambda_)\n",
    "            self.loss_history.append(loss)\n",
    "            end_time = time.process_time()\n",
    "            self.cpu_time_history.append(end_time - start_time)\n",
    "\n",
    "            # Compute convergence gap and check termination condition\n",
    "            gap = np.linalg.norm(self.beta - beta_prev) / (np.linalg.norm(beta_prev) if np.linalg.norm(beta_prev) != 0 else 1)\n",
    "            self.gap_history.append(gap)\n",
    "            if gap < self.tol:\n",
    "                break\n",
    "\n",
    "        return self.beta, self.loss_history, self.gap_history, self.cpu_time_history\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X and y are your data matrix and target vector\n",
    "lambda_ = 0.3\n",
    "lr = 0.01\n",
    "max_iter = 1000\n",
    "tol = 1e-6\n",
    "solver = AcceleratedLassoMethod(A, b, lambda_, lr, max_iter, tol)\n",
    "solution, loss_history, gap_history, cpu_time_history = solver.compute_steps()\n",
    "\n",
    "# Output results for analysis\n",
    "print(f\"Solution: {solution[:5]}...\")  # Showing first few elements of the solution\n",
    "print(f\"Loss history: {loss_history[:5]}...\")  # First few loss values\n",
    "print(f\"Gap history: {gap_history[:5]}...\")  # First few gaps\n",
    "print(f\"CPU time history: {cpu_time_history[:5]}...\")  # First few CPU times\n",
    "\n",
    "\n",
    "print(f\"Solution: {solution[:5]}...\")  # Showing first few elements of the solution\n",
    "print(f\"Iterations: {iterations}\")\n",
    "print(f\"Final Gap: {final_gap}\")\n",
    "print(f\"Indices of significant gap changes: {indices}\")\n",
    "print(f\"CPU time at significant changes: {cpu_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [ 0.  0.  0. -0.  0.]...\n",
      "Iterations: 10000\n",
      "Final Gap: 0.044637258385165045\n",
      "Indices of significant gap changes: [ 1  2  3  4 10 28]\n",
      "CPU time at significant changes: [0.015625 0.015625 0.046875 0.046875 0.078125 0.171875]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from math import sqrt\n",
    "\n",
    "def soft_thresholding(x, lambda_):\n",
    "    \"\"\"Apply soft thresholding for LASSO.\"\"\"\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - lambda_, 0.0)\n",
    "\n",
    "def lasso_objective(X, y, beta, lambda_):\n",
    "    \"\"\"Compute LASSO objective function.\"\"\"\n",
    "    n = len(y)\n",
    "    residual = y - X.dot(beta)\n",
    "    loss = 0.5 * np.dot(residual, residual) / n\n",
    "    penalty = lambda_ * np.linalg.norm(beta, ord=1)\n",
    "    return loss + penalty\n",
    "\n",
    "def lasso_gradient(X, y, beta, lambda_):\n",
    "    \"\"\"Compute gradient for LASSO.\"\"\"\n",
    "    n = len(y)\n",
    "    residual = y - X.dot(beta)\n",
    "    gradient_loss = -X.T.dot(residual) / n\n",
    "    gradient_penalty = lambda_ * np.sign(beta)\n",
    "    return gradient_loss + gradient_penalty\n",
    "\n",
    "def find_magnitude_indices(gap_history, func=np.log2):\n",
    "    \"\"\"Identify indices where the log magnitude of the gap changes significantly.\"\"\"\n",
    "    magnitudes = np.floor(func(gap_history))\n",
    "    unique_magnitudes = []\n",
    "    indices = []\n",
    "    for i, magnitude in enumerate(magnitudes):\n",
    "        if magnitude not in unique_magnitudes:\n",
    "            unique_magnitudes.append(magnitude)\n",
    "            indices.append(i + 1)  # Storing 1-based index for clarity\n",
    "    indices = np.array(indices)\n",
    "    return indices, unique_magnitudes\n",
    "\n",
    "class AcceleratedLassoMethod:\n",
    "    def __init__(self, X, y, lambda_, lr=0.01, max_iter=1000, tol=1e-6):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.lambda_ = lambda_\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.beta = np.zeros(X.shape[1])\n",
    "        self.t = 1\n",
    "        self.loss_history = []\n",
    "        self.gap_history = []\n",
    "        self.cpu_time_history = []\n",
    "\n",
    "    def compute_steps(self):\n",
    "        beta_prev = np.zeros_like(self.beta)\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            start_time = time.process_time()\n",
    "            t_prev = self.t\n",
    "            self.t = 0.5 * (1 + sqrt(1 + 4 * t_prev**2))\n",
    "            y_tilde = self.beta + ((t_prev - 1) / self.t) * (self.beta - beta_prev)\n",
    "            gradient = lasso_gradient(self.X, self.y, y_tilde, self.lambda_)\n",
    "            beta_prev = self.beta.copy()\n",
    "            self.beta = soft_thresholding(y_tilde - self.lr * gradient, self.lr * self.lambda_)\n",
    "            loss = lasso_objective(self.X, self.y, self.beta, self.lambda_)\n",
    "            self.loss_history.append(loss)\n",
    "            end_time = time.process_time()\n",
    "            self.cpu_time_history.append(end_time - start_time)\n",
    "\n",
    "            # Compute convergence gap and check termination condition\n",
    "            gap = np.linalg.norm(self.beta - beta_prev) / (np.linalg.norm(beta_prev) if np.linalg.norm(beta_prev) != 0 else 1)\n",
    "            self.gap_history.append(gap)\n",
    "            if gap < self.tol:\n",
    "                break\n",
    "\n",
    "        final_gap = self.gap_history[-1] if self.gap_history else None\n",
    "        iterations = i + 1\n",
    "        indices_of_changes, _ = find_magnitude_indices(self.gap_history)\n",
    "        cpu_times_at_changes = np.cumsum(self.cpu_time_history)[indices_of_changes - 1]  # Adjust for 0-based index\n",
    "\n",
    "        return self.beta, iterations, final_gap, indices_of_changes, cpu_times_at_changes\n",
    "\n",
    "\n",
    "lambda_ = 0.5\n",
    "solver = AcceleratedLassoMethod(A, b, lambda_, max_iter=10000, tol=1e-12)\n",
    "solution, iterations, final_gap, indices, cpu_time = solver.compute_steps()\n",
    "\n",
    "# Output results for analysis\n",
    "print(f\"Solution: {solution[:5]}...\")  # Showing first few elements of the solution\n",
    "print(f\"Iterations: {iterations}\")\n",
    "print(f\"Final Gap: {final_gap}\")\n",
    "print(f\"Indices of significant gap changes: {indices}\")\n",
    "print(f\"CPU time at significant changes: {cpu_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAG Solution (first 5 elements): [ 0.11190684 -0.35738204 -0.18666485 ... -0.02723769 -0.09114664\n",
      " -0.28201096]\n",
      "True Solution (first 5 elements): [0. 0. 0. ... 0. 0. 0.]\n",
      "Relative Gap History: [0.5345670844678049, 0.3301030680457549, 0.2259003333232547, 0.16543333448267675, 0.1267499759139684, 0.10021281181678422, 0.08106500704992278, 0.06671877830177501, 0.055654506613736054, 0.046924853248181, 0.03991034454104063, 0.034189683035946196, 0.02946673736206275, 0.025527420345436758, 0.022213180429108467, 0.019404134251082453, 0.01700800142361202, 0.014952643859415264, 0.013180909226067603, 0.011646986232023244, 0.010313776047255205, 0.009150962007484097, 0.008133569047566397, 0.007240873042508832, 0.006455564409291675, 0.005763099306197503, 0.005151191158436198, 0.00460940845518985, 0.0041288539276763325, 0.003701906675829155, 0.003322013430317374, 0.00298351848610559, 0.002681524303261869, 0.002411776597558518, 0.0021705691145686547, 0.001954664319942114, 0.001761227032675436, 0.0015877686400012322, 0.0014321000073061027, 0.0012922915674042517, 0.0011666393650711427, 0.001053636063272531, 0.0009519461007843517, 0.0008603843373222122, 0.0007778976398639995, 0.0007035489586792099, 0.0006365035184123974, 0.0005760168120899372, 0.0005214241370047894, 0.00047213145334648387, 0.00042760738096328207, 0.0003873761781844551, 0.000351011570316116, 0.00031813131515263124, 0.0002883924093337502, 0.00026148685320997826, 0.00023713790351650508, 0.0002150967529844533, 0.00019513958434079034, 0.00017706495322073644, 0.00016069146054161051, 0.00014585568003686462, 0.0001324103110615957, 0.00012022253057181545, 0.00010917252144588523, 9.915215713645869e-05, 9.006382508176064e-05, 8.181937342299103e-05, 7.433916741471614e-05, 6.755124351915747e-05, 6.139055057441622e-05, 5.579826865095408e-05, 5.072119728233764e-05, 4.611120569771867e-05, 4.192473850955579e-05, 3.812237103866706e-05, 3.4668409100059155e-05, 3.15305286390963e-05, 2.8679451108828035e-05, 2.6088650921775094e-05, 2.3734091702458944e-05, 2.1593988415005582e-05, 1.9648592749556275e-05, 1.787999942580857e-05, 1.6271971317122105e-05, 1.4809781515874102e-05, 1.3480070655464914e-05, 1.22707179770713e-05, 1.1170724784302636e-05, 1.0170109066924602e-05, 9.259810198487085e-06, 8.431602722869684e-06, 7.678018344404194e-06, 6.992275324219655e-06, 6.368214565508019e-06, 5.80024174113069e-06, 5.2832748817531365e-06, 4.812696899275947e-06, 4.3843125725789826e-06, 3.9943095688875914e-06, 3.6392231153042414e-06, 3.3159039734470252e-06, 3.0214894029261802e-06, 2.753376830628025e-06, 2.509199969644727e-06, 2.286807156647308e-06, 2.0842416984419074e-06, 1.8997240389378958e-06, 1.731635575418952e-06, 1.5785039696095015e-06, 1.438989813554789e-06, 1.3118745239160813e-06, 1.1960493499512763e-06, 1.0905053914597652e-06, 9.943245329241782e-07, 9.066712086622948e-07, 8.267849219775706e-07, 7.53973448472317e-07, 6.876066603010461e-07, 6.271109140828509e-07, 5.719639503226453e-07, 5.216902574246511e-07, 4.7585685749806465e-07, 4.340694750603718e-07, 3.959690538190204e-07, 3.6122858927930417e-07, 3.295502484874175e-07, 3.006627505817993e-07, 2.7431898433955836e-07, 2.502938410162198e-07, 2.2838224298743431e-07, 2.0839735028851457e-07, 1.9016892873379395e-07, 1.7354186523500457e-07, 1.583748166440595e-07, 1.4453898025103354e-07, 1.3191697460435675e-07, 1.2040182113966494e-07, 1.0989601688693313e-07, 1.0031069058363429e-07, 9.15648343612529e-08, 8.358460413703207e-08, 7.630268275710588e-08, 6.965770002712081e-08, 6.359370462351766e-08, 5.805968308123337e-08, 5.300912186063907e-08, 4.839960834579279e-08, 4.419246751460733e-08, 4.035243091172532e-08, 3.684733512724165e-08, 3.3647847107768894e-08, 3.0727213929801585e-08, 2.8061034709589262e-08, 2.5627052893864362e-08, 2.340496694049697e-08, 2.1376257798264153e-08, 1.9524031660272922e-08, 1.7832876744730867e-08, 1.6288732655352526e-08, 1.4878771373880163e-08, 1.3591288766797571e-08, 1.2415605714768706e-08, 1.1341977911577666e-08, 1.036151373100558e-08, 9.466099262585377e-09, 8.648330047086105e-09, 7.901448704893178e-09, 7.219288235017128e-09, 6.596220181582425e-09, 6.02710744244241e-09, 5.507261160093526e-09, 5.032401472547348e-09, 4.59862171334082e-09, 4.202355713085715e-09, 3.840348034813983e-09, 3.5096267715933334e-09, 3.2074787383766237e-09, 2.9314269300380965e-09, 2.679209809484609e-09, 2.4487625387220674e-09, 2.2381997852889484e-09, 2.0458000667962052e-09, 1.8699914610414638e-09, 1.7093385684560379e-09, 1.5625305886946628e-09, 1.4283704870899228e-09, 1.305765080148345e-09, 1.1937160052516302e-09, 1.0913114151672187e-09, 9.977185193529785e-10, 9.121767114723462e-10, 8.339911925456502e-10, 7.625273489931651e-10, 6.972055201297143e-10, 6.374961621678772e-10, 5.829155506071173e-10, 5.330217798184312e-10, 4.874111631168693e-10, 4.4571487916798424e-10, 4.0759597826851943e-10, 3.72746586190018e-10, 3.408854181320705e-10, 3.1175550187666325e-10, 2.8512196826123935e-10, 2.6077026049174263e-10, 2.385043200727122e-10, 2.1814492591718474e-10, 1.9952836051377991e-10, 1.8250499946969413e-10, 1.66938104872988e-10, 1.5270269830873288e-10, 1.3968456608151835e-10, 1.2777930239091738e-10, 1.1689150456264028e-10, 1.0693398846281947e-10, 9.782702050038046e-11, 8.949773933689855e-11, 8.187956543610342e-11, 7.491159815164612e-11, 6.853820122638437e-11, 6.270848392823864e-11]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_sparse_least_squares(m, n, rho):\n",
    "    A = np.random.uniform(-1, 1, (m, n))\n",
    "    x_star = np.zeros(n)\n",
    "    non_zero_indices = np.random.choice(n, int(n * rho), replace=False)\n",
    "    x_star[non_zero_indices] = np.random.normal(0, 1, int(n * rho))\n",
    "    b = np.dot(A, x_star)  # Assuming no noise for simplicity\n",
    "    return A, b, x_star\n",
    "\n",
    "def grad_f(x, A, b):\n",
    "    return A.T @ (A @ x - b)\n",
    "\n",
    "def nag_for_least_squares(A, b, x0, L0, max_iter=1000, epsilon=1e-6):\n",
    "    x_k = x0\n",
    "    y_k = x0\n",
    "    a_k = 0\n",
    "    L = L0\n",
    "    A_k = 0\n",
    "    initial_gap = np.linalg.norm(grad_f(x0, A, b))\n",
    "    \n",
    "    def T_L(y, L):\n",
    "        return y - grad_f(y, A, b) / L\n",
    "    \n",
    "    gap_history = []\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        a = (1 + np.sqrt(1 + 4 * L * A_k)) / (2 * L)\n",
    "        A_next = A_k + a\n",
    "        \n",
    "        y = (A_k * x_k + a * y_k) / A_next\n",
    "        T_L_y = T_L(y, L)\n",
    "        \n",
    "        while np.linalg.norm(grad_f(T_L_y, A, b) - grad_f(y, A, b)) > L * np.linalg.norm(T_L_y - y):\n",
    "            L *= 2  # Adjust L\n",
    "            T_L_y = T_L(y, L)\n",
    "        \n",
    "        x_next = T_L(y, L)\n",
    "        y_k = x_next\n",
    "        x_k = x_next\n",
    "        A_k = A_next\n",
    "        \n",
    "        current_gap = np.linalg.norm(grad_f(x_k, A, b))\n",
    "        relative_gap = current_gap / initial_gap\n",
    "        gap_history.append(relative_gap)\n",
    "        \n",
    "        if current_gap < epsilon:\n",
    "            break\n",
    "    \n",
    "    return x_k, gap_history\n",
    "\n",
    "# Example usage\n",
    "m, n, rho = 1000, 4000, 0.1\n",
    "A, b, x_star = generate_sparse_least_squares(m, n, rho)\n",
    "x0 = np.zeros(n)  # Initial point\n",
    "L0 = 1.0          # Initial Lipschitz constant guess\n",
    "\n",
    "solution_nag, gap_history = nag_for_least_squares(A, b, x0, L0)\n",
    "print(\"NAG Solution (first 5 elements):\", solution_nag[::])\n",
    "print(\"True Solution (first 5 elements):\", x_star[::])\n",
    "print(\"Relative Gap History:\", gap_history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
