{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_sparse_least_squares(m, n, rho):\n",
    "    \"\"\"\n",
    "    Generates a Sparse Least Squares problem according to the specifications in Nesterov's paper.\n",
    "\n",
    "    Parameters:\n",
    "    m (int): Number of rows in matrix A.\n",
    "    n (int): Number of columns in matrix A, n > m.\n",
    "    rho (float): Sparsity and magnitude control parameter.\n",
    "\n",
    "    Returns:\n",
    "    A (numpy.ndarray): Generated dense matrix A of shape (m, n).\n",
    "    b (numpy.ndarray): Generated vector b of shape (m,).\n",
    "    x_star (numpy.ndarray): Sparse solution x* of shape (n,).\n",
    "\n",
    "    Example:\n",
    "    m, n, rho = 10, 20, 0.1  # dimensions and sparsity level\n",
    "    A, b, x_star = generate_sparse_least_squares(m, n, rho)\n",
    "\n",
    "    # print(\"Matrix A:\\n\", A)\n",
    "    # print(\"Vector b:\\n\", b)\n",
    "    print(\"Sparse solution x*:\\n\", x_star)\n",
    "    \"\"\"\n",
    "    # Generate a dense matrix A with elements uniformly distributed in [-1, 1]\n",
    "    A = np.random.uniform(-1, 1, (m, n))\n",
    "\n",
    "    # Generate a sparse solution x_star\n",
    "    x_star = np.zeros(n)\n",
    "    # Ensure sparsity in the solution\n",
    "    non_zero_indices = np.random.choice(n, int(n * rho), replace=False)\n",
    "    x_star[non_zero_indices] = np.random.normal(0, 1, int(n * rho))\n",
    "\n",
    "    # Calculate the vector b = A*x_star + noise\n",
    "    # noise = np.random.normal(0, 0.1, m)  # adding small Gaussian noise\n",
    "    b = np.dot(A, x_star) # + noise\n",
    "\n",
    "    return A, b, x_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [-0.01874395  0.0236633   0.0107372  -0.02279081  0.07950713]...\n",
      "Iterations: 46\n",
      "Final Gap: 6.405778562169117e-10\n",
      "Indices of significant gap changes: [ 1  2  4  6 10 14 21 29 37 47]\n",
      "CPU time at significant changes: [1.953125 2.03125  2.03125  2.046875 2.09375  2.125    2.15625  2.203125\n",
      " 2.328125 2.515625]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class NesterovAcceleratedGradientMethod:\n",
    "    def __init__(self, A, b, L0, v0, max_iter=10000, tol=1e-6):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.L = L0\n",
    "        self.v = v0\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.gap_history = []\n",
    "        self.loss_history = []\n",
    "        self.cpu_time_history = []\n",
    "\n",
    "    def gradient_f(self, x):\n",
    "        return self.A.T @ (self.A @ x - self.b)\n",
    "\n",
    "    def f(self, x):\n",
    "        return np.linalg.norm(self.A @ x - self.b)**2 / 2\n",
    "\n",
    "    def nag_step(self, y, L):\n",
    "        grad = self.gradient_f(y)\n",
    "        return y - grad / L\n",
    "\n",
    "    def compute_steps(self):\n",
    "        v_prev = self.v\n",
    "        initial_loss = self.f(self.v)\n",
    "        \n",
    "        for k in range(self.max_iter):\n",
    "            start_time = time.process_time()\n",
    "\n",
    "            y = self.v + k / (k + 3) * (self.v - v_prev)\n",
    "            T_L_y = self.nag_step(y, self.L)\n",
    "\n",
    "            while np.linalg.norm(self.gradient_f(T_L_y) - self.gradient_f(y)) > self.L * np.linalg.norm(T_L_y - y):\n",
    "                self.L *= 1.1  # Increase L and recompute\n",
    "                T_L_y = self.nag_step(y, self.L)\n",
    "            \n",
    "            v_prev = self.v\n",
    "            self.v = T_L_y\n",
    "            \n",
    "            current_loss = self.f(self.v)\n",
    "            current_gap = current_loss / initial_loss\n",
    "\n",
    "            end_time = time.process_time()\n",
    "            \n",
    "            self.gap_history.append(current_gap)\n",
    "            self.loss_history.append(current_loss)\n",
    "            self.cpu_time_history.append(end_time - start_time)\n",
    "            \n",
    "            if current_gap < self.tol:\n",
    "                break\n",
    "\n",
    "        return self.v, k, current_gap, self.gap_history, self.loss_history, self.cpu_time_history\n",
    "\n",
    "# Example usage\n",
    "m, n, rho = 1000, 4000, 0.1\n",
    "A, b, x_star = generate_sparse_least_squares(m, n, rho)\n",
    "v0 = np.zeros(n)  # Initial point\n",
    "L0 = 1.0          # Initial Lipschitz constant guess\n",
    "\n",
    "solver = NesterovAcceleratedGradientMethod(A, b, L0, v0, max_iter=1000, tol=1e-9)\n",
    "solution_nag, iterations_nag, final_gap_nag, gap_history_nag, loss_history_nag, cpu_time_nag = solver.compute_steps()\n",
    "\n",
    "# Analyzing gap magnitudes and CPU time\n",
    "def find_magnitude_indices(gap_history, func=np.log10):\n",
    "    magnitudes = np.floor(func(gap_history))\n",
    "    unique_magnitudes = []\n",
    "    indices = []\n",
    "    for i, magnitude in enumerate(magnitudes):\n",
    "        if magnitude not in unique_magnitudes:\n",
    "            unique_magnitudes.append(magnitude)\n",
    "            indices.append(i + 1)\n",
    "    return np.array(indices), unique_magnitudes\n",
    "\n",
    "indices_nag, magnitudes_nag = find_magnitude_indices(gap_history_nag, func=np.log10)\n",
    "cpu_time_nag = np.array(cpu_time_nag).cumsum()[indices_nag - 1]\n",
    "\n",
    "# Output results for analysis\n",
    "print(f\"Solution: {solution_nag[:5]}...\")  # Showing first few elements of the solution\n",
    "print(f\"Iterations: {iterations_nag}\")\n",
    "print(f\"Final Gap: {final_gap_nag}\")\n",
    "print(f\"Indices of significant gap changes: {indices_nag}\")\n",
    "print(f\"CPU time at significant changes: {cpu_time_nag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class DualGradientMethod:\n",
    "    def __init__(self, A, b, penalty, gamma_u, gamma_d, L_0, v_0, max_iter=10000, tol=1e-6) -> None:\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.penalty = penalty\n",
    "        self.gamma_u = gamma_u\n",
    "        self.gamma_d = gamma_d\n",
    "        self.L = L_0\n",
    "        self.v = v_0\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.gap_history = []\n",
    "        self.loss_history = []\n",
    "        self.cpu_time_history = []\n",
    "\n",
    "    def gradient_f(self, x):\n",
    "        return 2 * np.dot(self.A.T, (np.dot(self.A, x) - self.b))\n",
    "\n",
    "    def f(self, x):\n",
    "        return np.linalg.norm(np.dot(self.A, x) - self.b)**2 / 2\n",
    "\n",
    "    def psi(self, x):\n",
    "        # Placeholder for potential regularizer function\n",
    "        return 0\n",
    "\n",
    "    def compute_steps(self):\n",
    "        v_prev = self.v\n",
    "        initial_residual = np.dot(self.A, self.v) - self.b\n",
    "        initial_loss = np.linalg.norm(initial_residual)**2 / 2\n",
    "        \n",
    "        for k in range(self.max_iter):\n",
    "            start_time = time.process_time()\n",
    "\n",
    "            # Nesterov acceleration\n",
    "            if k > 0:\n",
    "                y = self.v + (k - 1) / (k + 2) * (self.v - v_prev)\n",
    "            else:\n",
    "                y = self.v\n",
    "\n",
    "            grad_f = self.gradient_f(y)\n",
    "            Mk = np.linalg.norm(grad_f, 2)\n",
    "            if Mk == 0:\n",
    "                Mk = 1e-16  # Prevent division by zero\n",
    "\n",
    "            L_k = max(self.L, Mk / self.gamma_d)\n",
    "            T, L_new = self.gradient_iteration(self.penalty, self.gamma_u, y, L_k)\n",
    "            self.L = L_new\n",
    "            \n",
    "            v_prev = self.v\n",
    "            self.v = T\n",
    "            \n",
    "            current_residual = np.dot(self.A, self.v) - self.b\n",
    "            current_loss = np.linalg.norm(current_residual)**2 / 2\n",
    "            current_gap = current_loss / initial_loss\n",
    "\n",
    "            end_time = time.process_time()\n",
    "\n",
    "            self.gap_history.append(current_gap)\n",
    "            self.loss_history.append(current_loss)\n",
    "            self.cpu_time_history.append(end_time - start_time)\n",
    "\n",
    "            if current_gap < self.tol:\n",
    "                break\n",
    "\n",
    "        return self.v, k, current_gap, self.gap_history, self.loss_history, self.cpu_time_history\n",
    "\n",
    "    def gradient_iteration(self, penalty, gamma_u, x, M):\n",
    "        L = M\n",
    "        while True:\n",
    "            changes = self.gradient_f(x) - L * x\n",
    "            T = three_cases(changes, penalty, L)\n",
    "            if not self.psi(T) > np.dot(self.gradient_f(T), (x - T)) + np.linalg.norm(x - T)**2 * L / 2 + self.psi(x):\n",
    "                break\n",
    "            L *= gamma_u\n",
    "        return T, L\n",
    "\n",
    "def three_cases(changes, penalty, denominator):\n",
    "    # Vectorized handling of three cases for the proximal gradient update\n",
    "    return (np.less_equal(changes, -penalty) * (-changes - penalty) + np.greater_equal(changes, penalty) * (-changes + penalty)) / denominator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [-2.80889804e-06 -1.14807672e-07  2.08181017e-06  4.13181627e-07\n",
      " -2.71063769e-01]...\n",
      "Iterations: 9999\n",
      "Final Gap: 0.00010730729782251724\n",
      "Indices of significant gap changes: [   1    2    3    5    6    7    8    9   11   12   13   16   20   23\n",
      "   26   29   36   39   43   61 1079]\n",
      "CPU time at significant changes: [ 0.        0.046875  0.046875  0.046875  0.0625    0.078125  0.078125\n",
      "  0.09375   0.09375   0.09375   0.140625  0.203125  0.265625  0.265625\n",
      "  0.3125    0.375     0.40625   0.46875   0.484375  0.71875  14.6875  ]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "m, n = 1000, 4000  # dimensions\n",
    "rho = 0.1          # sparsity factor\n",
    "tol = 1e-9         # tolerance for relative gap reduction\n",
    "\n",
    "# Generate the problem\n",
    "A, b, x_star = generate_sparse_least_squares(m, n, rho)\n",
    "\n",
    "# Instantiate and solve using DualGradientMethod\n",
    "v0 = np.zeros(n)\n",
    "L0 = 2\n",
    "gamma_d = 2\n",
    "penalty = 0.5  # This replaces lambda_ which was likely a regularization parameter\n",
    "\n",
    "solver = DualGradientMethod(A, b, penalty, gamma_d=2, gamma_u=1.1, L_0=L0, v_0=v0, max_iter=10000, tol=tol)\n",
    "solution_dual, iterations_dual, final_gap_dual, gap_history_dual, loss_history_dual, cpu_history_dual = solver.compute_steps()\n",
    "\n",
    "# Analyzing gap magnitudes and CPU time\n",
    "def find_magnitude_indices(gap_history, func=np.log10):\n",
    "    magnitudes = np.floor(func(gap_history))\n",
    "    unique_magnitudes = []\n",
    "    indices = []\n",
    "    for i, magnitude in enumerate(magnitudes):\n",
    "        if magnitude not in unique_magnitudes:\n",
    "            unique_magnitudes.append(magnitude)\n",
    "            indices.append(i + 1)\n",
    "    indices = np.array(indices)\n",
    "    return indices, unique_magnitudes\n",
    "\n",
    "indices_dual, magnitudes_dual = find_magnitude_indices(gap_history_dual, func=np.log2)\n",
    "cpu_history_dual = np.array(cpu_history_dual).cumsum()[indices_dual - 1]  # Adjusting indices for Python's 0-based index\n",
    "\n",
    "# Output results for analysis\n",
    "print(f\"Solution: {solution_dual[:5]}...\")  # Showing first few elements of the solution\n",
    "print(f\"Iterations: {iterations_dual}\")\n",
    "print(f\"Final Gap: {final_gap_dual}\")\n",
    "print(f\"Indices of significant gap changes: {indices_dual}\")\n",
    "print(f\"CPU time at significant changes: {cpu_history_dual}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [-0.11493664  0.22963825  0.14850047  0.23265202  0.07101003]...\n",
      "Iterations: 9999\n",
      "Final Gap: 7.286066231851823e-05\n",
      "Indices of significant gap changes: [  1  28  48  64  76  85  92  97 101 104 106 108 109 110 111 112 113 115\n",
      " 120]\n",
      "CPU time at significant changes: [0.       0.109375 0.1875   0.203125 0.25     0.265625 0.40625  0.4375\n",
      " 0.453125 0.453125 0.46875  0.46875  0.46875  0.515625 0.515625 0.515625\n",
      " 0.515625 0.53125  0.5625  ]\n"
     ]
    }
   ],
   "source": [
    "class PrimalGradientMethod:\n",
    "    def __init__(self, A, b, x0, tol, max_iter=10000):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.x = x0\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.gap_history = []\n",
    "        self.loss_history = []\n",
    "        self.cpu_time_history = []\n",
    "\n",
    "    def gradient_f(self, x):\n",
    "        return 2 * np.dot(self.A.T, (np.dot(self.A, x) - self.b))\n",
    "\n",
    "    def f(self, x):\n",
    "        return np.linalg.norm(np.dot(self.A, x) - self.b)**2\n",
    "\n",
    "    def compute_steps(self):\n",
    "        initial_loss = self.f(self.x)\n",
    "        self.loss_history.append(initial_loss)\n",
    "        \n",
    "        for k in range(self.max_iter):\n",
    "            start_time = time.process_time()\n",
    "            gradient = self.gradient_f(self.x)\n",
    "            step_size = 0.1 / (np.linalg.norm(gradient) if np.linalg.norm(gradient) != 0 else 1e-16)\n",
    "            self.x -= step_size * gradient\n",
    "            current_loss = self.f(self.x)\n",
    "            relative_gap = current_loss / initial_loss\n",
    "\n",
    "            self.loss_history.append(current_loss)\n",
    "            self.gap_history.append(relative_gap)\n",
    "            end_time = time.process_time()\n",
    "            self.cpu_time_history.append(end_time - start_time)\n",
    "\n",
    "            if relative_gap < self.tol:\n",
    "                break\n",
    "\n",
    "        return self.x, k, relative_gap, self.gap_history, self.loss_history, self.cpu_time_history\n",
    "\n",
    "# Parameters\n",
    "m, n = 1000, 4000  # dimensions\n",
    "rho = 0.1          # sparsity factor\n",
    "tol = 1e-9         # tolerance for relative gap reduction\n",
    "\n",
    "# Generate the problem\n",
    "A, b, x_star = generate_sparse_least_squares(m, n, rho)\n",
    "x0 = np.zeros(n)\n",
    "\n",
    "# Instantiate and solve using PrimalGradientMethod\n",
    "solver = PrimalGradientMethod(A, b, x0, tol, max_iter=10000)\n",
    "solution, iterations, final_gap, gap_history, loss_history, cpu_time = solver.compute_steps()\n",
    "\n",
    "# Analyzing gap magnitudes and CPU time\n",
    "def find_magnitude_indices(gap_history, func=np.log2):\n",
    "    magnitudes = np.floor(func(gap_history))\n",
    "    unique_magnitudes = []\n",
    "    indices = []\n",
    "    for i, magnitude in enumerate(magnitudes):\n",
    "        if magnitude not in unique_magnitudes:\n",
    "            unique_magnitudes.append(magnitude)\n",
    "            indices.append(i + 1)\n",
    "    indices = np.array(indices)\n",
    "    return indices, unique_magnitudes\n",
    "\n",
    "indices, magnitudes = find_magnitude_indices(gap_history)\n",
    "cpu_time = np.array(cpu_time).cumsum()[indices - 1]  # Adjusting indices for Python's 0-based index\n",
    "\n",
    "# Output results for analysis\n",
    "print(f\"Solution: {solution[:5]}...\")  # Showing first few elements of the solution\n",
    "print(f\"Iterations: {iterations}\")\n",
    "print(f\"Final Gap: {final_gap}\")\n",
    "print(f\"Indices of significant gap changes: {indices}\")\n",
    "print(f\"CPU time at significant changes: {cpu_time}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
